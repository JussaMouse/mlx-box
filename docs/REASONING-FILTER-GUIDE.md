# Reasoning Filter Guide

## The Problem

Qwen3-Thinking models generate reasoning chains to improve answer quality. This reasoning appears in responses as:

**Modern API format (Qwen3-Thinking-2507):**
```json
{
  "choices": [{
    "message": {
      "content": "The answer is 4.",
      "reasoning": "Let me think about this... 2+2 equals 4 in standard arithmetic..."
    }
  }]
}
```

**Legacy format (some models):**
```
<think>Let me think about this... 2+2 equals 4...</think>
The answer is 4.
```

Sometimes you want to **hide the reasoning** from end users while **keeping the quality benefits**.

---

## Two Filtering Approaches

### Option 1: Proxy-Level Filtering ‚úÖ RECOMMENDED

**What it does:**
- Model generates reasoning (improves answer quality)
- Auth proxy strips `reasoning` field before sending to client
- Clean separation, no behavior change

**Advantages:**
- ‚úÖ Model still reasons (better quality)
- ‚úÖ Clean API responses (no reasoning clutter)
- ‚úÖ Works with modern reasoning format
- ‚úÖ Can be toggled per-service
- ‚úÖ No prompt engineering needed

**Configuration:**
```toml
[services.thinking]
filter_reasoning = true  # Strip reasoning field at proxy layer
```

**How it works:**
1. Client requests ‚Üí Auth proxy ‚Üí MLX backend
2. Backend generates response with reasoning
3. Auth proxy strips `message.reasoning` field
4. Client receives clean response with only `content`

**Implementation:** `models/auth-proxy.py` (already implemented above)

---

### Option 2: Prompt-Level Filtering

**What it does:**
- Changes the prompt template to instruct model NOT to generate reasoning
- Uses `--chat-template-args '{"enable_thinking":false}'`

**Advantages:**
- ‚úÖ Saves compute (no reasoning generated)
- ‚úÖ Faster responses (fewer tokens)
- ‚úÖ Saves bandwidth

**Disadvantages:**
- ‚ö†Ô∏è May reduce quality (no reasoning to guide answers)
- ‚ö†Ô∏è Doesn't work with Qwen3-Thinking-2507 (model ignores instruction)
- ‚ö†Ô∏è Changes model behavior

**Configuration:**
```toml
[services.thinking]
disable_thinking_tags = true  # Requires mlx-lm >= 0.30.6
```

**How it works:**
1. Chat template omits `<think>` tag from prompt
2. Model doesn't see the thinking instruction
3. Model outputs answer directly (no reasoning)

---

## Which Should You Use?

### Use Proxy-Level Filtering (Option 1) if:
- ‚úÖ You want **best quality** answers
- ‚úÖ You're using **Qwen3-Thinking-2507** (modern format)
- ‚úÖ You want to **hide reasoning** from clients
- ‚úÖ You have **enough compute** to generate reasoning
- ‚úÖ You want **flexibility** to enable/disable without model restarts

### Use Prompt-Level Filtering (Option 2) if:
- ‚úÖ You're using **base Qwen3 models** (not Thinking variant)
- ‚úÖ You need **fastest possible** responses
- ‚úÖ You're **compute-constrained**
- ‚úÖ Quality loss is acceptable

### Use Both Together if:
- ‚úÖ You want **defense in depth** (prompt + proxy filtering)
- ‚úÖ Model might output reasoning despite prompt instruction

---

## Configuration Examples

### Hide reasoning (recommended):
```toml
[services.thinking]
filter_reasoning = true         # Proxy strips reasoning field
# disable_thinking_tags = false # Let model generate reasoning
```

### Show reasoning (debugging):
```toml
[services.thinking]
filter_reasoning = false        # Pass reasoning to client
# disable_thinking_tags = false # Let model generate reasoning
```

### Maximum speed (lower quality):
```toml
[services.thinking]
filter_reasoning = false        # No filtering needed
disable_thinking_tags = true    # Don't generate reasoning at all
```

---

## Testing

### Test if filtering works:

```bash
# Test with filtering disabled (should see reasoning)
curl http://127.0.0.1:8081/v1/chat/completions \
  -H "Authorization: Bearer YOUR_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Qwen3-30B-A3B-Thinking-2507-4bit",
    "messages": [{"role": "user", "content": "What is 2+2?"}],
    "max_tokens": 200
  }' | jq '.choices[0].message'

# Expected output (filtering disabled):
{
  "role": "assistant",
  "content": "The answer is 4.",
  "reasoning": "Let me think... 2+2 equals 4..."
}

# Expected output (filtering enabled):
{
  "role": "assistant",
  "content": "The answer is 4."
}
```

---

## How to Enable

### Step 1: Update Configuration

Edit `config/settings.toml`:

```toml
[services.thinking]
filter_reasoning = true  # Add this line
```

### Step 2: Restart Service

```bash
sudo launchctl kickstart -k system/com.mlx-box.thinking
```

### Step 3: Verify

Check logs for confirmation:

```bash
tail ~/Library/Logs/com.mlx-box.thinking/stderr.log
# Should see: "üß† Reasoning filter enabled - 'reasoning' field will be stripped from responses"
```

### Step 4: Test

Send a test request and verify no `reasoning` field in response.

---

## Performance Impact

### Proxy-Level Filtering:
- **CPU overhead:** ~0.1ms per response (negligible)
- **Memory overhead:** None
- **Latency impact:** None (same generation time)
- **Quality impact:** None (reasoning still generated)

### Prompt-Level Filtering:
- **Speed improvement:** 20-50% faster (fewer tokens to generate)
- **Memory savings:** ~30-50% less KV cache (shorter responses)
- **Quality impact:** 10-30% quality loss (no reasoning guidance)

---

## Troubleshooting

### Reasoning still appears in responses

**Check 1:** Is filtering enabled in config?
```bash
grep filter_reasoning config/settings.toml
```

**Check 2:** Did you restart the service?
```bash
sudo launchctl list | grep thinking
```

**Check 3:** Check logs for confirmation message
```bash
tail ~/Library/Logs/com.mlx-box.thinking/stderr.log | grep "Reasoning filter"
```

### Model outputs `<think>` tags instead of `reasoning` field

This means you're using an older model or format. Solutions:
- Update to Qwen3-Thinking-2507 (modern format)
- Use prompt-level filtering instead (`disable_thinking_tags = true`)
- Implement tag-based streaming filter (more complex)

---

## API Format Reference

### Modern Format (Qwen3-Thinking-2507)
```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "Final answer here",
      "reasoning": "Thinking process here"  // ‚Üê This field gets stripped
    }
  }],
  "usage": {
    "completion_tokens": 150,
    "prompt_tokens": 20,
    "total_tokens": 170
  }
}
```

### Legacy Format (Older Models)
```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "<think>Thinking process</think>\nFinal answer here"
    }
  }]
}
```

---

## Best Practice Recommendations

1. **Use proxy-level filtering** for production (best quality)
2. **Keep reasoning enabled** during development (helps debugging)
3. **Log reasoning** to a separate channel for monitoring
4. **Monitor token usage** to track reasoning overhead
5. **A/B test** with and without reasoning to measure quality impact

---

**Last Updated:** 2025-02-15
**Applies To:** mlx-lm >= 0.30.6, Qwen3-Thinking-2507 models
