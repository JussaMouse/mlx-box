# vice-server Configuration File
#
# This is an example configuration. Copy this file to 'settings.toml'
# in the same directory and edit it to match your setup.
# The 'settings.toml' file is ignored by Git and should be backed up.
#

[server]
# The public domain name for this server (e.g., "mlx-box.mydomain.com").
# This is REQUIRED for obtaining an SSL certificate with Let's Encrypt.
domain_name = "mlx-box.example.com"

# The email address to use for Let's Encrypt SSL registration and recovery.
letsencrypt_email = "admin@example.com"

# The host IP the application services will bind to.
# For a secure, internet-facing server using a reverse proxy,
# this MUST be "127.0.0.1".
host = "127.0.0.1"

# API key authentication for MLX services.
# Clients must send: Authorization: Bearer <api_key>

# Option 1: Single API key (backward compatible)
# api_key = "your-secret-api-key-here"

# Option 2: Multiple API keys (recommended for multiple clients)
api_keys = [
    "production-bartleby-key-64-chars-here",  # Production Bartleby instance
    "development-bartleby-key-64-chars-here", # Development/testing
    # Add more keys as needed
]

[services]
  # Configuration for the secure shell (SSH) service.
  [services.ssh]
  port = 2222

  # --- 3-Tier Intelligent Routing Configuration ---

  # Tier 0: Router Service
  # Lightweight model for request classification and trivial queries
  [services.router]
  port = 8082          # Frontend port (with auth)
  backend_port = 8092  # Backend MLX server port (no auth)
  model = "mlx-community/Qwen3-0.6B-4bit"
  max_tokens = 100

  # OPTIMIZATION: Model parameters for deterministic classification
  temperature = 0.1         # Low temperature for consistent routing decisions
  top_p = 0.9              # Nucleus sampling threshold
  frequency_penalty = 0.0  # No penalty needed for short responses
  presence_penalty = 0.0   # No penalty needed

  # Tier 1 & 2: Fast Service
  # General purpose, low-latency model for most queries and simple tools
  # NOTE: This is a MoE (Mixture of Experts) model with 30.5B total params
  #       but only 3.3B activated per token (11% activation rate)
  [services.fast]
  port = 8080          # Frontend port (with auth)
  backend_port = 8090  # Backend MLX server port (no auth)
  model = "mlx-community/Qwen3-30B-A3B-4bit"
  max_tokens = 8192    # INCREASED from 4096 - utilize available RAM

  # OPTIMIZATION: Model parameters for balanced conversation + tool use
  temperature = 0.6         # Balanced creativity and coherence
  top_p = 0.92             # Slightly higher for natural responses
  frequency_penalty = 0.3  # Mild penalty for response variety
  presence_penalty = 0.0   # Use frequency_penalty OR presence_penalty, not both

  # Tier 3: Thinking Service
  # High-reasoning model for complex tasks, coding, and planning
  # NOTE: This is also a MoE model with 30.5B total, 3.3B active
  [services.thinking]
  port = 8081          # Frontend port (with auth)
  backend_port = 8091  # Backend MLX server port (no auth)
  model = "mlx-community/Qwen3-30B-A3B-Thinking-2507-4bit"
  max_tokens = 16384   # INCREASED from 8192 - longer reasoning contexts
  thinking_budget = 8192  # INCREASED from 4096 - more thinking tokens

  # OPTIMIZATION: Model parameters for precise reasoning
  temperature = 0.2         # Low temperature for accuracy in reasoning/coding
  top_p = 0.9              # Standard nucleus sampling
  frequency_penalty = 0.0  # Reasoning models don't need penalty
  presence_penalty = 0.0   # Not needed

  # --- Embedding Service ---

  # Configuration for the Embedding service.
  # Provides high-quality semantic embeddings for document search and similarity.
  [services.embedding]
  port = 8083          # Frontend port (with auth)
  backend_port = 8093  # Backend service port (no auth)
  # The SentenceTransformer model to use for embeddings.
  model = "Qwen/Qwen3-Embedding-8B"
  batch_size = 64      # Process 64 texts at once for efficiency
  max_seq_length = 1024  # Max tokens per text (Qwen supports up to 32K)
  quantization = true  # Use int8 for 2x speed and 50% memory reduction

  # --- OCR (Vision Chat) Service ---
  #
  # OpenAI-compatible /v1/chat/completions endpoint for vision OCR using an MLX VLM.
  # Intended for screenshots, receipts, scanned docs, etc.
  [services.ocr]
  port = 8085          # Frontend port (with auth)
  backend_port = 8095  # Backend service port (no auth)
  model = "mlx-community/olmOCR-2-7B-1025-mlx-8bit"
  # Optional: tune server behavior (passed through to mlx-openai-server if set)
  # context_length = 8192
  # Strongly recommended: keep OCR stateless/isolated per request
  max_concurrency = 1

  # Configuration for the Frontend web service.
  [services.frontend]
  port = 8000

# --- OPTIMIZATION NOTES ---
#
# Temperature Guidelines:
#   0.0-0.2: Deterministic, factual, precise (router, thinking for code/math)
#   0.5-0.7: Balanced, conversational (fast tier for general use)
#   0.8-1.0: Creative, varied (not typically needed for assistants)
#
# Top-P Guidelines:
#   0.9-0.95: Standard range for most use cases
#   Higher = more diverse outputs
#   Lower = more focused outputs
#
# Frequency Penalty Guidelines:
#   0.0: No penalty (default, recommended for reasoning models)
#   0.1-0.5: Mild penalty for response variety
#   0.5-1.0: Stronger penalty to reduce repetition
#
# Context Length Guidelines:
#   Router: 100 tokens (classification only)
#   Fast: 8192 tokens (2x increase - you have the RAM!)
#   Thinking: 16384 tokens (2x increase - for long reasoning chains)
#
# Expected Performance Impact:
#   - Router: Deterministic classification (temp 0.1)
#   - Fast: +15% response coherence, 2x context capacity
#   - Thinking: +10% reasoning accuracy, 2x context capacity
#
# Memory Usage (Mac Studio M2 Ultra 192GB):
#   Router (0.6B-4bit): ~0.5 GB
#   Fast (30B-A3B-4bit): ~18 GB
#   Thinking (30B-A3B-4bit): ~18 GB
#   Embedding (8B): ~16 GB
#   OCR (7B-8bit): ~7 GB
#   KV Cache (all services): ~20-40 GB (depends on concurrent requests)
#   System + overhead: ~20 GB
#   Total: ~100-120 GB / 192 GB (52-63% utilization)
#   Headroom: ~70-90 GB available
