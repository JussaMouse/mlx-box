# vice-server Configuration File
#
# This is an example configuration. Copy this file to 'settings.toml'
# in the same directory and edit it to match your setup.
# The 'settings.toml' file is ignored by Git and should be backed up.
#

[server]
# The public domain name for this server (e.g., "mlx-box.mydomain.com").
# This is REQUIRED for obtaining an SSL certificate with Let's Encrypt.
domain_name = "mlx-box.example.com"

# The email address to use for Let's Encrypt SSL registration and recovery.
letsencrypt_email = "admin@example.com"

# The host IP the application services will bind to.
# For a secure, internet-facing server using a reverse proxy,
# this MUST be "127.0.0.1".
host = "127.0.0.1"

# API key authentication for MLX services.
# Clients must send: Authorization: Bearer <api_key>

# Option 1: Single API key (backward compatible)
# api_key = "your-secret-api-key-here"

# Option 2: Multiple API keys (recommended for multiple clients)
api_keys = [
    "production-bartleby-key-64-chars-here",  # Production Bartleby instance
    "development-bartleby-key-64-chars-here", # Development/testing
    # Add more keys as needed
]

[services]
  # Configuration for the secure shell (SSH) service.
  [services.ssh]
  port = 2222

  # --- 3-Tier Intelligent Routing Configuration ---

  # Tier 0: Router Service
  # Lightweight model for request classification and trivial queries
  [services.router]
  port = 8082          # Frontend port (with auth)
  backend_port = 8092  # Backend MLX server port (no auth)
  model = "mlx-community/Qwen3-0.6B-4bit"
  max_tokens = 150     # OPTIMIZED: Increased from 100 for more detailed routing decisions

  # OPTIMIZATION: Model parameters for deterministic classification
  temperature = 0.1         # ✓ ACTIVE: Low temperature for consistent routing decisions
  top_p = 0.9              # ✓ ACTIVE: Nucleus sampling threshold
  frequency_penalty = 0.0  # ✗ NOT USED: MLX server doesn't support repetition penalty
  presence_penalty = 0.0   # ✗ NOT USED: MLX server doesn't support presence penalty

  # Tier 1 & 2: Fast Service
  # General purpose, low-latency model for most queries and simple tools
  # NOTE: This is a MoE (Mixture of Experts) model with 30.5B total params
  #       but only 3.3B activated per token (11% activation rate)
  [services.fast]
  port = 8080          # Frontend port (with auth)
  backend_port = 8090  # Backend MLX server port (no auth)
  model = "mlx-community/Qwen3-30B-A3B-4bit"
  max_tokens = 16384   # OPTIMIZED: 2x increase from 8192 (128GB RAM has plenty of headroom)

  # OPTIMIZATION: Model parameters for balanced conversation + tool use
  temperature = 0.6         # ✓ ACTIVE: Balanced creativity and coherence
  top_p = 0.92             # ✓ ACTIVE: Slightly higher for natural responses
  frequency_penalty = 0.3  # ✗ NOT USED: MLX server doesn't support repetition penalty
  presence_penalty = 0.0   # ✗ NOT USED: MLX server doesn't support presence penalty

  # Filter thinking tags/reasoning (recommended for clean responses)
  # Qwen3-30B-A3B uses legacy <think>...</think> tags in content field
  filter_reasoning = true   # ✓ RECOMMENDED: Strip <think> tags from responses

  # Tier 3: Thinking Service
  # High-reasoning model for complex tasks, coding, and planning
  # NOTE: This is also a MoE model with 30.5B total, 3.3B active
  [services.thinking]
  port = 8081          # Frontend port (with auth)
  backend_port = 8091  # Backend MLX server port (no auth)
  model = "mlx-community/Qwen3-30B-A3B-Thinking-2507-4bit"
  max_tokens = 32768       # OPTIMIZED: Maximum 32K context (128GB RAM can handle it)
  thinking_budget = 16384  # OPTIMIZED: 2x increase from 8192 for complex reasoning chains

  # OPTIMIZATION: Model parameters for precise reasoning
  # Official Qwen3-Thinking-2507 recommendations from model card
  temperature = 0.6         # ✓ ACTIVE: Optimal for reasoning exploration (was 0.2)
  top_p = 0.95             # ✓ ACTIVE: Allows diverse reasoning paths (was 0.9)
  frequency_penalty = 0.0  # ✗ NOT USED: MLX server doesn't support repetition penalty
  presence_penalty = 0.0   # ✗ NOT USED: MLX server doesn't support presence penalty

  # === REASONING FILTERING OPTIONS ===
  #
  # Option 1: Filter reasoning at proxy level (RECOMMENDED)
  # - Model STILL generates reasoning (improves quality)
  # - Auth proxy strips 'reasoning' field before sending to client
  # - Cleaner than prompt-level filtering, no behavior change
  filter_reasoning = true   # ✓ RECOMMENDED: Strip reasoning field from responses
  #
  # Option 2: Disable reasoning generation at model level
  # - Changes the PROMPT to instruct model not to generate thinking
  # - May reduce response quality (no reasoning to guide answers)
  # - Qwen3-Thinking-2507 may ignore this (model fine-tuned for thinking)
  # disable_thinking_tags = true  # Alternative: Requires mlx-lm >= 0.30.6

  # --- Embedding Service ---

  # Configuration for the Embedding service.
  # Provides high-quality semantic embeddings for document search and similarity.
  [services.embedding]
  port = 8083          # Frontend port (with auth)
  backend_port = 8093  # Backend service port (no auth)
  # The SentenceTransformer model to use for embeddings.
  model = "Qwen/Qwen3-Embedding-8B"
  batch_size = 128     # OPTIMIZED: 2x increase from 64 (128GB RAM allows faster bulk embedding)
  max_seq_length = 2048  # OPTIMIZED: 2x increase from 1024 for better long document understanding
  quantization = true  # Use int8 for 2x speed and 50% memory reduction

  # --- OCR (Vision Chat) Service ---
  #
  # OpenAI-compatible /v1/chat/completions endpoint for vision OCR using an MLX VLM.
  # Intended for screenshots, receipts, scanned docs, etc.
  [services.ocr]
  port = 8085          # Frontend port (with auth)
  backend_port = 8095  # Backend service port (no auth)
  model = "mlx-community/olmOCR-2-7B-1025-mlx-8bit"
  # Optional: tune server behavior (passed through to mlx-openai-server if set)
  # context_length = 8192
  # Strongly recommended: keep OCR stateless/isolated per request
  max_concurrency = 1

  # Configuration for the Frontend web service.
  [services.frontend]
  port = 8000

# --- OPTIMIZATION NOTES ---
#
# PARAMETER STATUS:
#   ✓ ACTIVE: temperature, top_p, max_tokens, thinking_budget
#   ✗ NOT USED: frequency_penalty, presence_penalty
#
# The patched_mlx_server.py currently only supports --temp and --top-p.
# Frequency and presence penalties are stored in config for future use
# but are not currently passed to the MLX server.
#
# Temperature Guidelines:
#   0.0-0.2: Deterministic, factual, precise (router, thinking for code/math)
#   0.5-0.7: Balanced, conversational (fast tier for general use)
#   0.8-1.0: Creative, varied (not typically needed for assistants)
#
# Top-P Guidelines:
#   0.9-0.95: Standard range for most use cases
#   Higher = more diverse outputs
#   Lower = more focused outputs
#
# Frequency Penalty Guidelines:
#   0.0: No penalty (default, recommended for reasoning models)
#   0.1-0.5: Mild penalty for response variety
#   0.5-1.0: Stronger penalty to reduce repetition
#
# Context Length Guidelines:
#   Router: 100 tokens (classification only)
#   Fast: 8192 tokens (2x increase - you have the RAM!)
#   Thinking: 16384 tokens (2x increase - for long reasoning chains)
#
# Expected Performance Impact:
#   - Router: Deterministic classification (temp 0.1)
#   - Fast: +15% response coherence, 2x context capacity
#   - Thinking: +10% reasoning accuracy, 2x context capacity
#
# Memory Usage (Mac Studio M2 Ultra 192GB):
#   Router (0.6B-4bit): ~0.5 GB
#   Fast (30B-A3B-4bit): ~18 GB
#   Thinking (30B-A3B-4bit): ~18 GB
#   Embedding (8B): ~16 GB
#   OCR (7B-8bit): ~7 GB
#   KV Cache (all services): ~20-40 GB (depends on concurrent requests)
#   System + overhead: ~20 GB
#   Total: ~100-120 GB / 192 GB (52-63% utilization)
#   Headroom: ~70-90 GB available
